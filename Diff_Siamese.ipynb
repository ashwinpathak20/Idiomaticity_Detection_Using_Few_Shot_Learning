{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gXdCtnjq1PK",
        "outputId": "cc7100a6-ae5e-4ce1-b73e-95e48c87d4d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SemEval_2022_Task2-idiomaticity' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ljCJSq3om",
        "outputId": "45e9168a-e6cd-404c-8baf-24429b8f3430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AStitchInLanguageModels' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1OczNFNq4_7",
        "outputId": "6963b678-1753-4631-c1c9-ea68afd0da91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "/content/transformers\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.14.0.dev0) (0.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.14.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.14.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.14.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.14.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.14.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.14.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.14.0.dev0) (1.1.0)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.14.0.dev0\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed transformers-4.14.0.dev0\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!pip install --editable .\n",
        "%cd /content/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVN0vrZTq6oX",
        "outputId": "dd16b4f4-8a18-4cbd-fa1a-c4f514068407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.16.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jj_uaj3aq8Od"
      },
      "outputs": [],
      "source": [
        "import site\n",
        "site.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gSUai3g0q-Ji"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jst76TcOq_kX"
      },
      "outputs": [],
      "source": [
        "def load_csv( path, delimiter=',' ) : \n",
        "  header = None\n",
        "  data   = list()\n",
        "  with open( path, encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader( csvfile, delimiter=delimiter ) \n",
        "    for row in reader : \n",
        "      if header is None : \n",
        "        header = row\n",
        "        continue\n",
        "      data.append( row ) \n",
        "  return header, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P6SA-GvKrBSY"
      },
      "outputs": [],
      "source": [
        "def write_csv( data, location ) : \n",
        "  with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer( csvfile ) \n",
        "    writer.writerows( data ) \n",
        "  print( \"Wrote {}\".format( location ) ) \n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KHJHosMVVNXS"
      },
      "outputs": [],
      "source": [
        "class Node():\n",
        "  def __init__(self, sentence, label):\n",
        "    self.sentence = sentence\n",
        "    self.label = label\n",
        "\n",
        "def create_idiom_dict_train(data_location, file_name) :\n",
        "    idiom_dict = {}\n",
        "    file_name = os.path.join( data_location, file_name ) \n",
        "    header, data = load_csv( file_name )\n",
        "    for elem in data:\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence = elem[ header.index( 'Target' ) ]\n",
        "        idiom = elem[ header.index( 'MWE' ) ]\n",
        "        if idiom in idiom_dict:\n",
        "          idiom_dict[idiom].append(Node(sentence, label))\n",
        "        else:\n",
        "          idiom_dict[idiom] = [Node(sentence, label)]\n",
        "    return idiom_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xWO_TnshqY21"
      },
      "outputs": [],
      "source": [
        "d1 = create_idiom_dict_train('SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', 'train_zero_shot.csv')\n",
        "d2 = create_idiom_dict_train('SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', 'train_one_shot.csv')\n",
        "for key, value in d2.items():\n",
        "  if key in d1:\n",
        "    d1[key].append(value)\n",
        "  else:\n",
        "    d1[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JvkkIptrrDsB"
      },
      "outputs": [],
      "source": [
        "def _get_train_data( data_location, file_name, include_context, include_idiom ) :\n",
        "    \n",
        "    file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    out_header = [ 'label1', 'label2', 'sentence1', 'sentence3' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label1', 'label2', 'sentence1', 'sentence2', 'sentence3', 'sentence4' ]\n",
        "        \n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem1 in data :\n",
        "        label     = elem1[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem1[ header.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem1[ header.index( 'Previous' ) ], elem1[ header.index( 'Target' ) ], elem1[ header.index( 'Next' ) ] ] )\n",
        "        for elem2 in d1[elem1[ header.index( 'MWE' ) ]]:\n",
        "          if elem2.sentence != sentence1:\n",
        "              label2     = elem2.label\n",
        "              sentence2 = elem2.sentence\n",
        "              this_row = None\n",
        "              if not include_idiom :\n",
        "                  this_row = [ label, label2, sentence1, sentence2 ] \n",
        "              else :\n",
        "                  sentence3 = elem1[ header.index( 'MWE' ) ]\n",
        "                  sentence4 = sentence3\n",
        "                  this_row = [ label, label2, sentence1, sentence3, sentence2, sentence4]\n",
        "              out_data.append( this_row )\n",
        "              assert len( out_header ) == len( this_row )\n",
        "    return [ out_header ] + out_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rMWvYWX8rGsu"
      },
      "outputs": [],
      "source": [
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None : \n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "    \n",
        "    out_header = [ 'label1', 'label2', 'sentence1', 'sentence3' ]\n",
        "    if include_idiom :\n",
        "        out_header = [ 'label1', 'label2', 'label3', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'sentence6' ]\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None : \n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "            \n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        this_row = None\n",
        "        if not include_idiom :\n",
        "            this_row = [ label, sentence1 ] \n",
        "        else :\n",
        "            sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "            this_row = [ label, sentence1, sentence2 ]\n",
        "        idiom = elem[ input_headers.index( 'MWE' ) ]\n",
        "        other_nodes = d1[idiom]\n",
        "        if(len(other_nodes)==1):\n",
        "            if not include_idiom :\n",
        "                this_row = [ label, other_nodes[0].label, sentence1, other_nodes[0].sentence ] \n",
        "            else :\n",
        "                sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "                this_row = [ label, other_nodes[0].label, other_nodes[0].label, sentence1, sentence2, other_nodes[0].sentence, sentence2, other_nodes[0].sentence, sentence2 ]\n",
        "        else:\n",
        "            if not include_idiom :\n",
        "                this_row = [ label, other_nodes[0].label, sentence1, other_nodes[0].sentence ] \n",
        "            else :\n",
        "                sentence2 = elem[ input_headers.index( 'MWE' ) ]\n",
        "                this_row = [ label, other_nodes[0].label, other_nodes[1].label, sentence1, sentence2, other_nodes[0].sentence, sentence2, other_nodes[1].sentence, sentence2 ]\n",
        "           \n",
        "        assert len( out_header ) == len( this_row ) \n",
        "        out_data.append( this_row )\n",
        "        \n",
        "\n",
        "    return [ out_header ] + out_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U4bPrXRRrJ6H"
      },
      "outputs": [],
      "source": [
        "def create_data( input_location, output_location ) :\n",
        "\n",
        "    \n",
        "    ## Zero shot data\n",
        "    train_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = True,\n",
        "        include_idiom   = False\n",
        "    )\n",
        "    write_csv( train_data, os.path.join( output_location, 'ZeroShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context  = True,\n",
        "        include_idiom    = False\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "\n",
        "    ## OneShot Data (combine both for training)\n",
        "    train_zero_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "    train_one_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "\n",
        "    assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "    train_data = train_one_data + train_zero_data[1:]\n",
        "    write_csv( train_data, os.path.join( output_location, 'OneShot', 'train.csv' ) )\n",
        "    \n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'OneShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'OneShot', 'eval.csv' ) )\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7t8zqdt9zAQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c73ed788-bcaf-4be5-94ca-1e465e27103a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"eval_data = _get_dev_eval_data(\\n        data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\\n        input_file_name  = 'eval.csv',\\n        gold_file_name   = None,\\n        include_context  = False,\\n        include_idiom    = True\\n    )\""
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train_zero_data = _get_train_data(\n",
        "        data_location   = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "train_one_data = _get_train_data(\n",
        "        data_location   = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "        file_name       = 'train_one_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True\n",
        "    )\n",
        "\n",
        "assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "train_data = train_one_data + train_zero_data[1:]\n",
        "\n",
        "dev_data = _get_dev_eval_data(\n",
        "        data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )        \n",
        "    \n",
        "\"\"\"eval_data = _get_dev_eval_data(\n",
        "        data_location    = 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/',\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True\n",
        "    )\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnJSMleDrO64",
        "outputId": "8c3c5673-5c04-4524-a689-e4180e6d86dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AStitchInLanguageModels  sample_data\t\t\t  transformers\n",
            "Data\t\t\t SemEval_2022_Task2-idiomaticity\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AW_HqBIFrPbE"
      },
      "outputs": [],
      "source": [
        "outpath = 'Data'\n",
        "\n",
        "Path( os.path.join( outpath, 'ZeroShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "Path( os.path.join( outpath, 'OneShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', outpath )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "183UBr4ArRSh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import logging\n",
        "\n",
        "from typing          import Optional\n",
        "from dataclasses     import dataclass, field\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "from datasets        import load_dataset, load_metric\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.utils         import check_min_version\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qtZiIQ5SrXBd"
      },
      "outputs": [],
      "source": [
        "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
        "check_min_version(\"4.6.0.dev0\")\n",
        "\n",
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0EdhT3XrZCu",
        "outputId": "27644a44-d5b6-4972-99c8-0cf2bf532f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['label1', 'label2', 'sentence1', 'sentence2', 'sentence3', 'sentence4'], ['0', '1', 'Program leaders said the scholarship defines public service broadly and imagines a variety of pathways toward civic engagement.', 'public service', 'In the ensuing years, Wennberg might not have managed to knock down the parking deck, but his administration helped keep Central Vermont Public Service from moving its corporate headquarters out of the city, and after successfully fighting a number of shopping centers city officials worried would pose a threat to downtown, he negotiated a deal that kept Diamond Run Mall from hosting a movie theater or supermarket and got the city a couple million dollars in payments that funded a variety of projects through the years.', 'public service']]\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QJsCAgjzt-9",
        "outputId": "dec7d695-5692-480f-a848-9e5579e9456a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original input: Williams stars as a talk-radio personality named Gebriel.\n",
            "\n",
            "Pre-process in separate steps.\n",
            "Tokenized input: ['[CLS]', 'williams', 'stars', 'as', 'a', 'talk', '-', 'radio', 'personality', 'named', 'ge', '##bri', '##el', '.', '[SEP]']\n",
            "BERT input_ids: [101, 3766, 3340, 2004, 1037, 2831, 1011, 2557, 6180, 2315, 16216, 23736, 2884, 1012, 102]\n",
            "BERT input_ids padded: tensor([  101,  3766,  3340,  2004,  1037,  2831,  1011,  2557,  6180,  2315,\n",
            "        16216, 23736,  2884,  1012,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "\n",
            "Pre-process in one step.\n",
            "BERT input_ids padded: tensor([  101,  3766,  3340,  2004,  1037,  2831,  1011,  2557,  6180,  2315,\n",
            "        16216, 23736,  2884,  1012,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "org_input = \"Williams stars as a talk-radio personality named Gebriel.\"\n",
        "\n",
        "print(\"Original input:\", org_input)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased',\n",
        "                                          cache_dir=None,\n",
        "                                          use_fast=True,\n",
        "                                          revision=\"main\",\n",
        "                                          use_auth_token=None,)\n",
        "\n",
        "# Pre-process in two steps. \n",
        "print(\"\\nPre-process in separate steps.\")\n",
        "\n",
        "# 1. Split a word into multiple sub-words, e.g. 'Gebriel' -> 'ge', '##bri', '##el'\n",
        "sub_word_list = tokenizer.tokenize(f\"{tokenizer.cls_token} {org_input} {tokenizer.sep_token} \")\n",
        "print(\"Tokenized input:\", sub_word_list)\n",
        "\n",
        "# 2. Pad the sentence to a fixed/max length\n",
        "max_len = 128\n",
        "sub_word_idx_list = tokenizer.convert_tokens_to_ids(sub_word_list)\n",
        "print(\"BERT input_ids:\", sub_word_idx_list)\n",
        "\n",
        "input_ids = pad_sequences([sub_word_idx_list], maxlen=max_len, \n",
        "                          value=tokenizer.pad_token_id, padding=\"post\", \n",
        "                          dtype=\"long\", truncating=\"post\")\n",
        "input_ids = torch.tensor(input_ids)\n",
        "print(\"BERT input_ids padded:\", input_ids[0])\n",
        "\n",
        "\n",
        "# Pre-process in one step (check out https://huggingface.co/transformers/preprocessing.html#base-use for more details). \n",
        "print(\"\\nPre-process in one step.\")\n",
        "\n",
        "encoded_input = tokenizer(org_input, padding='max_length', max_length = max_len, \n",
        "                          truncation=True, return_tensors=\"pt\")\n",
        "print('BERT input_ids padded:', encoded_input['input_ids'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_data(data):\n",
        "    indices = list(range(len(data)))\n",
        "    random.shuffle(indices)\n",
        "    shuffled_data = []\n",
        "    for i in indices:\n",
        "        shuffled_data.append(data[i])\n",
        "    return shuffled_data"
      ],
      "metadata": {
        "id": "rxajbXyYtpJp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nUGKP2VK0PY6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "def preproces(input, tokenizer, max_len, batch_size, data_class=\"train\"):\n",
        "    input1 = []\n",
        "    input2 = []\n",
        "    label1 = []\n",
        "    label2 = []\n",
        "    for i in input:\n",
        "      \"\"\"if(i[1]!='1' and i[1]!='0'):\n",
        "        continue\"\"\"\n",
        "      label1.append(int(i[0]))\n",
        "      label2.append(int(i[1]))\n",
        "      args = (\n",
        "            (i[2], i[3])\n",
        "      )\n",
        "      input1.append(args)\n",
        "      args = (\n",
        "            (i[4], i[5])\n",
        "      )\n",
        "      input2.append(args)\n",
        "    encoded_input1 = tokenizer(input1, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n",
        "    encoded_input2 = tokenizer(input2, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    input_ids1 = encoded_input1['input_ids']\n",
        "    attention_mask1 = encoded_input1['attention_mask']\n",
        "    labels1 = torch.tensor(label1)\n",
        "\n",
        "    #print(input_ids1.size(), attention_mask1.size(), labels1.size())\n",
        "\n",
        "    input_ids2 = encoded_input2['input_ids']\n",
        "    attention_mask2 = encoded_input2['attention_mask']\n",
        "    labels2 = torch.tensor(label2)\n",
        "\n",
        "    #print(input_ids2.size(), attention_mask2.size(), labels2.size())\n",
        "    dataset_tensor = TensorDataset(input_ids1, attention_mask1, labels1, input_ids2, attention_mask2, labels2)\n",
        "\n",
        "    if data_class == \"train\":\n",
        "        sampler = RandomSampler(dataset_tensor)\n",
        "    else:\n",
        "        sampler = SequentialSampler(dataset_tensor)\n",
        "    dataloader = DataLoader(dataset_tensor, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "    return dataloader\n",
        "#train_dataloader = preproces(train_data[1:], tokenizer, max_len, batch_size, data_class=\"train\")\n",
        "#dev_dataloader = preproces(eval_data[1:], tokenizer, max_len, batch_size, data_class=\"dev\")\n",
        "#test_dataloader = preproces(dev_Data[1:], tokenizer, max_len, batch_size, data_class=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uIP79MjDtx5r"
      },
      "outputs": [],
      "source": [
        "max_len = 512\n",
        "batch_size = 32\n",
        "def preproces_dev(input, tokenizer, max_len, batch_size, data_class=\"dev\"):\n",
        "    input1 = []\n",
        "    input2 = []\n",
        "    input3 = []\n",
        "    label1 = []\n",
        "    label2 = []\n",
        "    label3 = []\n",
        "    for i in input:\n",
        "      \"\"\"if(i[1]!='1' and i[1]!='0'):\n",
        "        continue\"\"\"\n",
        "      label1.append(int(i[0]))\n",
        "      label2.append(int(i[1]))\n",
        "      label3.append(int(i[2]))\n",
        "      args = (\n",
        "            (i[3], i[4])\n",
        "      )\n",
        "      input1.append(args)\n",
        "      args = (\n",
        "            (i[5], i[6])\n",
        "      )\n",
        "      input2.append(args)\n",
        "      args = (\n",
        "          (i[7], i[8])\n",
        "      )\n",
        "      input3.append(args)\n",
        "    encoded_input1 = tokenizer(input1, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n",
        "    encoded_input2 = tokenizer(input2, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n",
        "    encoded_input3 = tokenizer(input3, padding='max_length', max_length = max_len, truncation=True, return_tensors=\"pt\")\n",
        "    input_ids1 = encoded_input1['input_ids']\n",
        "    attention_mask1 = encoded_input1['attention_mask']\n",
        "    labels1 = torch.tensor(label1)\n",
        "    print(input_ids1.size(), attention_mask1.size(), labels1.size())\n",
        "\n",
        "    input_ids2 = encoded_input2['input_ids']\n",
        "    attention_mask2 = encoded_input2['attention_mask']\n",
        "    labels2 = torch.tensor(label2)\n",
        "\n",
        "    print(input_ids2.size(), attention_mask2.size(), labels2.size())\n",
        "\n",
        "    input_ids3 = encoded_input3['input_ids']\n",
        "    attention_mask3 = encoded_input3['attention_mask']\n",
        "    labels3 = torch.tensor(label3)\n",
        "\n",
        "    print(input_ids3.size(), attention_mask3.size(), labels3.size())    \n",
        "\n",
        "    dataset_tensor = TensorDataset(input_ids1, attention_mask1, labels1, input_ids2, attention_mask2, labels2, input_ids3, attention_mask3, labels3)\n",
        "\n",
        "    if data_class == \"train\":\n",
        "        sampler = RandomSampler(dataset_tensor)\n",
        "    else:\n",
        "        sampler = SequentialSampler(dataset_tensor)\n",
        "    dataloader = DataLoader(dataset_tensor, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "    return dataloader\n",
        "#dev_dataloader = preproces_dev(dev_data[1:], tokenizer, max_len, batch_size, data_class=\"dev\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GoS9OP9e0IcN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class SiameseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseModel, self).__init__()\n",
        "        \n",
        "        self.base_model = AutoModel.from_pretrained(\n",
        "            'distilbert-base-uncased',\n",
        "            from_tf=bool(\".ckpt\" in 'distilbert-base-uncased'),\n",
        "            config=config,\n",
        "            cache_dir=None,\n",
        "            revision=\"main\",\n",
        "            use_auth_token=None,\n",
        "        ).cuda()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.linear = nn.Linear(768, 2).cuda() # output features from bert is 768 and 2 is ur number of labels\n",
        "        \n",
        "    def forward(self, input_ids1, attn_mask1, input_ids2, attn_mask2):\n",
        "        #outputs1 = self.base_model(input_ids1, attention_mask=attn_mask1)[1]\n",
        "        #outputs2 = self.base_model(input_ids2, attention_mask=attn_mask2)[1]\n",
        "        outputs1 = self.base_model(input_ids1, attention_mask=attn_mask1).last_hidden_state[:, 0]\n",
        "        outputs2 = self.base_model(input_ids2, attention_mask=attn_mask2).last_hidden_state[:, 0]\n",
        "        difference = torch.abs(outputs1-outputs2).cuda()\n",
        "        # You write you new head here\n",
        "        outputs = self.dropout(difference)\n",
        "        outputs = self.linear(outputs)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "05DDBBjOYzrs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def Eval(bert_model, dataloader):\n",
        "\n",
        "    bert_model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    num_correct = 0\n",
        "    \n",
        "    for step, batch in enumerate(tqdm(dataloader)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            logits1 = nn.functional.softmax(bert_model.forward(batch[0], batch[1], batch[3], batch[4]), -1)\n",
        "        with torch.no_grad():\n",
        "            logits2 = nn.functional.softmax(bert_model.forward(batch[0], batch[1], batch[6], batch[7]), -1)\n",
        "        logits = torch.cat((logits1, logits2), dim=1)\n",
        "        max_args = torch.argmax(logits, dim=1)\n",
        "        batch_predictions = []\n",
        "        batch_true_labels = batch[2]\n",
        "        first_sentence_labels = batch[5]\n",
        "        second_sentence_labels = batch[8]\n",
        "        for idx, instance in enumerate(max_args):\n",
        "          if instance == 0:\n",
        "            batch_predictions.append((first_sentence_labels[idx] - 1) * -1) # 0, 1 toggle\n",
        "          elif instance == 1:\n",
        "            batch_predictions.append(first_sentence_labels[idx])\n",
        "          elif instance == 2:\n",
        "            batch_predictions.append((second_sentence_labels[idx] - 1) * -1)\n",
        "          else:\n",
        "            batch_predictions.append(second_sentence_labels[idx])\n",
        "        predictions += batch_predictions\n",
        "        true_labels += batch_true_labels\n",
        "    return true_labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jYDcfd5eY2oR"
      },
      "outputs": [],
      "source": [
        "def metrics(true_labels, predictions):\n",
        "    pre = []\n",
        "    tl = []\n",
        "    num_correct = 0\n",
        "    for pred, true_label in zip(predictions, true_labels):\n",
        "        pre.append(int(pred.item()))\n",
        "        tl.append(int(true_label.item()))\n",
        "        if pred == true_label:\n",
        "            num_correct += 1\n",
        "    print(\"\\nAccuracy: %s\" % (float(num_correct) / float(len(true_labels))))\n",
        "    print(\"F1 Score \")\n",
        "    print(f1_score(tl, pre, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "np7YcOhL4IWQ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def Train(bert_model, train_data, lr, n_epoch, tokenizer, batch_size, max_len):\n",
        "\n",
        "    print(\"Start Training!\")\n",
        "    optimizer = AdamW(bert_model.parameters(), lr=lr)\n",
        "    bert_model.train()\n",
        "    dev_dataloader = preproces_dev(dev_data[1:], tokenizer, max_len, batch_size, data_class=\"dev\")\n",
        "    # TRAIN loop\n",
        "    for epoch in range(n_epoch):\n",
        "        shuffled_train_data = shuffle_data(train_data)\n",
        "        shuffled_train_data = preproces(shuffled_train_data, tokenizer, max_len, batch_size)\n",
        "        print(f\"\\nEpoch {epoch}\")\n",
        "        torch.cuda.empty_cache()\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        for step, batch in enumerate(tqdm(shuffled_train_data)):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            bert_model.zero_grad()\n",
        "            # forward pass\n",
        "            logits = bert_model.forward(batch[0], batch[1], batch[3], batch[4])\n",
        "            # print(loss)\n",
        "            loss = 0\n",
        "            target = torch.where(batch[2]==batch[5], 1, 0)\n",
        "            #target = target.reshape(-1,1)\n",
        "            loss = nn.functional.cross_entropy(logits, target)\n",
        "            \n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            # track train loss\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_steps += 1\n",
        "            #loss = loss.detach()\n",
        "            # update parameters\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "        # print train loss per epoch\n",
        "        print(\"Train loss on epoch {}: {}\\n\".format(epoch, tr_loss / nb_tr_steps))\n",
        "\n",
        "        # Dev set evaluation\n",
        "        #print(\"Evaluate on the dev set:\")\n",
        "        #Eval(bert_model, dev_data)\n",
        "        true_labels, predictions = Eval(bert_model, dev_dataloader)\n",
        "        metrics(true_labels, predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oAKW6e6m5IHy"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "        'distilbert-base-uncased',\n",
        "        num_labels=2,\n",
        "        finetuning_task=None,\n",
        "        cache_dir=None,\n",
        "        revision=\"main\",\n",
        "        use_auth_token=None,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ar64c-jf4rp-",
        "outputId": "653d76fb-12fe-40ed-eeae-ff58d4a650b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training!\n",
            "torch.Size([739, 256]) torch.Size([739, 256]) torch.Size([739])\n",
            "torch.Size([739, 256]) torch.Size([739, 256]) torch.Size([739])\n",
            "torch.Size([739, 256]) torch.Size([739, 256]) torch.Size([739])\n",
            "\n",
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [34:24<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 0: 0.2011976760312391\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.7834912043301759\n",
            "F1 Score \n",
            "0.7832812797841611\n",
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:45<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 1: 0.027914294478812433\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.8078484438430311\n",
            "F1 Score \n",
            "0.806441735896944\n",
            "\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:45<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 2: 0.004176470857595012\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.7875507442489851\n",
            "F1 Score \n",
            "0.7857365784113302\n",
            "\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:45<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 3: 0.002549775099160543\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.803788903924222\n",
            "F1 Score \n",
            "0.8030908606448175\n",
            "\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:46<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 4: 0.0036464585212244503\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.7997293640054127\n",
            "F1 Score \n",
            "0.7986138730537511\n",
            "\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:46<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 5: 0.0022297952965727582\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.7726657645466847\n",
            "F1 Score \n",
            "0.7717424877553722\n",
            "\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:46<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 6: 0.0008306110766325563\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.7753721244925575\n",
            "F1 Score \n",
            "0.7746090190199018\n",
            "\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:47<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 7: 0.004905273601054784\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.774018944519621\n",
            "F1 Score \n",
            "0.7728944806658606\n",
            "\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2694/2694 [33:45<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss on epoch 8: 6.479127600087562e-05\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 24/24 [00:12<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.7672530446549392\n",
            "F1 Score \n",
            "0.766307785082881\n",
            "\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%| | 2232/2694 [27:59<05:47,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-2cf9e4ea0e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-3fc1e9358624>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(bert_model, train_data, lr, n_epoch, tokenizer, batch_size, max_len)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# track train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = SiameseModel()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "learning_rate = 2e-5\n",
        "num_epoch = 4\n",
        "torch.cuda.empty_cache()\n",
        "max_len = 256\n",
        "batch_size = 32\n",
        "\n",
        "if n_gpu > 1:\n",
        "    model.to(device)\n",
        "    model = torch.nn.DataParallel(model)\n",
        "else:\n",
        "    model.cuda()\n",
        "\n",
        "Train(model, train_data[1:], learning_rate, num_epoch, tokenizer, batch_size, max_len)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Diff_Siamese.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}